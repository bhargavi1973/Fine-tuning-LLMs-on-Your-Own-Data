{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjRvI3KSZv1wDRa534NRLj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhargavi1973/Fine-tuning-LLMs-on-Your-Own-Data/blob/main/FineTuneLLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Large Language Models like BERT, RoBERTa, and GPT have been pretrained on massive text corpora. But when it comes to solving specific tasks, like classifying customer reviews, summarizing legal documents, or tagging support tickets, you’ll want the model to learn from your data. So, in this article, I’ll take you through a complete guide to fine-tuning LLMs on your own data using Hugging Face Transformers.\n"
      ],
      "metadata": {
        "id": "6ZX7plNQPiUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import datasets\n",
        "import torch\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "ZjgiY7egP7-4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Library Versions ===\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Datasets version: {datasets.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amt07uv3P1Bs",
        "outputId": "8e959bb7-fbe2-4cf8-993d-03cfb783befe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Library Versions ===\n",
            "Transformers version: 4.57.1\n",
            "Datasets version: 4.0.0\n",
            "PyTorch version: 2.8.0+cu126\n",
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Load a Dataset Using TensorFlow Datasets\n",
        "\n",
        " In this step, we’ll use TensorFlow Datasets (TFDS) to load the ag_news_subset dataset, a popular text classification benchmark containing news headlines categorized into four classes (World, Sports, Business, Sci/Tech):"
      ],
      "metadata": {
        "id": "lzRQIk-FPzJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "ds, info = tfds.load('ag_news_subset', with_info=True, as_supervised=True)\n",
        "\n",
        "train_ds, test_ds = ds['train'], ds['test']\n",
        "\n",
        "for text, label in train_ds.take(1):\n",
        "    print(\"Text:\", text.numpy().decode())\n",
        "    print(\"Label:\", label.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prYLNn1gT71u",
        "outputId": "faa5d6d3-d4e8-4ed7-8e0d-cb2ce81b6757"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.\n",
            "Label: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Preprocess the Data\n",
        "Now, the next step is to prepare the data for input into our language model. Pretrained models like BERT expect text in a specific tokenized format, where each word or subword is converted into a numerical ID using the same tokenizer that was used during the model’s original training.\n",
        "\n",
        "This step ensures consistency between how the model was trained and how it will now see your custom data:"
      ],
      "metadata": {
        "id": "_QCznX0PQDbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "def tokenize_batch(texts, labels):\n",
        "    tokens = tokenizer(list(texts), padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    tokens[\"labels\"] = labels\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "9RX2yCwkbNIN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code above, we load the tokenizer associated with bert-base-uncased and define a helper function to tokenize a batch of text examples. This function also applies padding and truncation to keep sequence lengths uniform, something crucial for efficient training and batching.\n",
        "\n",
        "With the tokenizer ready, our next task is to convert the TensorFlow Datasets (TFDS) format into something that can be directly used by the Hugging Face model, which expects PyTorch-style inputs"
      ],
      "metadata": {
        "id": "r2gj3fg7QJwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since TFDS provides the data as TensorFlow tf.data.Dataset objects, we need to extract the raw text and labels, convert them to NumPy arrays, and then tokenize them using our helper function:"
      ],
      "metadata": {
        "id": "2UGBU01HQRFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def tfds_to_torch(dataset):\n",
        "    texts, labels = [], []\n",
        "    for text, label in tfds.as_numpy(dataset):\n",
        "        texts.append(text.decode())\n",
        "        labels.append(label)\n",
        "    return tokenize_batch(texts, torch.tensor(labels))\n",
        "\n",
        "train_encodings = tfds_to_torch(train_ds)\n",
        "test_encodings = tfds_to_torch(test_ds)"
      ],
      "metadata": {
        "id": "QZsOGJwnaWOY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step bridges the gap between data loading and model training by transforming our dataset into tokenized tensors, complete with input IDs, attention masks, and labels, all formatted for efficient use with PyTorch and Hugging Face’s Trainer API."
      ],
      "metadata": {
        "id": "wvcLt3f3QY7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Create a PyTorch Dataset\n",
        "\n",
        "At this point, we’ve tokenized our text data and structured it into tensors, but to feed it into a PyTorch training loop or the Hugging Face Trainer, we need to wrap it inside a custom Dataset class. This class acts as a bridge between the raw tokenized data and the model, allowing us to define how batches are loaded during training:"
      ],
      "metadata": {
        "id": "lZuSReMaQc_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class AGNewsDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "train_dataset = AGNewsDataset(train_encodings)\n",
        "test_dataset = AGNewsDataset(test_encodings)"
      ],
      "metadata": {
        "id": "UZFIDUcAabSZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By implementing __len__ and __getitem__, we enable PyTorch to efficiently index, shuffle, and batch the data. The code above creates this custom dataset class and initializes it for both the training and test sets, preparing our data for seamless integration with the training pipeline."
      ],
      "metadata": {
        "id": "8PAa6daLQi7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Load the Pretrained Model for Classification\n",
        "\n",
        "Now that our dataset is tokenized and wrapped in a PyTorch-compatible format, it’s time to load the pretrained language model that we’ll fine-tune. In this case, we’re using bert-base-uncased, a widely used version of BERT that has been trained on a large corpus of English text.\n",
        "\n",
        "Since our task is text classification with four output classes (news categories), we’ll load the model with a classification head on top and specify num_labels=4:"
      ],
      "metadata": {
        "id": "KmuToyz7QmvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWmBWk-Jae8g",
        "outputId": "9d8483d4-cb91-40c4-dd95-86294bcc44da"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Set Up the Training Loop\n",
        "With the model, tokenizer, and datasets ready, the next step is to configure how the training process should run. Instead of manually writing a training loop, we’ll leverage Hugging Face’s Trainer API, which handles everything from optimization to evaluation and logging.\n",
        "\n",
        "To do this, we first define TrainingArguments, where we specify essential training parameters like batch size, number of epochs, evaluation strategy, and logging frequency. These settings give us control over the model’s learning behaviour and how often we monitor its progress:"
      ],
      "metadata": {
        "id": "ZMlx7cqGQsg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIRkz2zmakKa",
        "outputId": "28776fbf-a293-4b76-eb6d-6a93ac2bc7f8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-770828445.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Training the Model\n",
        "\n",
        "With everything configured and the model ready, it’s time to kick off the training process. By calling the trainer.train(), we begin fine-tuning the pretrained BERT model on our custom dataset, allowing it to learn patterns specific to our text classification task:"
      ],
      "metadata": {
        "id": "P_XmQSx6QyLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./fine-tuned-bert-agnews\")\n",
        "tokenizer.save_pretrained(\"./fine-tuned-bert-agnews\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkCnkP0Fa41m",
        "outputId": "a613ec1d-5c52-4249-a62a-bb8d5fc91b1b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./fine-tuned-bert-agnews/tokenizer_config.json',\n",
              " './fine-tuned-bert-agnews/special_tokens_map.json',\n",
              " './fine-tuned-bert-agnews/vocab.txt',\n",
              " './fine-tuned-bert-agnews/added_tokens.json',\n",
              " './fine-tuned-bert-agnews/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}